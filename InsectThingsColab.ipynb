{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1zcQrp0prydJSfG0VhPdYgPyLC2u6cw0U",
      "authorship_tag": "ABX9TyMHUsZWGHrdAfI9U7v8Cnjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stdunson/InsectDetectionProject/blob/main/InsectThingsColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Transfer Learning with Large Scale Insect Datasets\n",
        "\n",
        "#Inconsistencies\n",
        "1. Not every data picture has every taxonomic level, so there has to be a way to filter that during gathering. The lowest level that every image consistently has is order, which is really high. We're gonna focus on family-level\n",
        "2. One dataset is JSON and the other is raw images. Shouldn't matter too much buecause I'm gonna have two different training algorithms\n",
        "3. Bioscan has 934 families while Insect Foundation has 1189(1500 according to code)\n",
        "\n"
      ],
      "metadata": {
        "id": "8wEHouczwKh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installs\n",
        "!pip3 install gdown\n",
        "!pip3 install wget\n",
        "!pip3 install utils\n",
        "!pip3 install bioscan-dataset"
      ],
      "metadata": {
        "id": "wszb2y6VDq31",
        "outputId": "0a5969cc-7492-4526-fc81-df03f09a1507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=5c5b2087730007d1a919b85c8ee2f960bce9e72f8e3c17764f48e4252aa46659\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=831ff363688f5ff579aee59e0ce54d1962660c9ed47cfdefc5e3e1a63d0d715a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/a1/81/1036477786ae0e17b522f6f5a838f9bc4288d1016fc5d0e1ec\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n",
            "Collecting bioscan-dataset\n",
            "  Downloading bioscan_dataset-1.3.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from bioscan-dataset) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bioscan-dataset) (2.2.2)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from bioscan-dataset) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from bioscan-dataset) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from bioscan-dataset) (0.24.0+cu126)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->bioscan-dataset) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->bioscan-dataset) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->bioscan-dataset) (2025.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->bioscan-dataset) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->bioscan-dataset) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->bioscan-dataset) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->bioscan-dataset) (3.0.3)\n",
            "Downloading bioscan_dataset-1.3.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: bioscan-dataset\n",
            "Successfully installed bioscan-dataset-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If using drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "isaICYyAlrl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94872f4a-8dea-45b9-85d8-d15f7941a2c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic Imports\n",
        "import os\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "#Doanloading things\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import gdown\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "#Pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Resnet\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n"
      ],
      "metadata": {
        "id": "Fp4Wn6s-ClKh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Downloading"
      ],
      "metadata": {
        "id": "CbWZ_uY3ETvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "47WTiIiawIHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83adbd97-ba5e-4772-afe1-8f3a5c585207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File missing: /root/Datasets/bioscan-5m/bioscan5m/metadata/csv/BIOSCAN_5M_Insect_Dataset_metadata.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.07G/2.07G [23:40<00:00, 1.45MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image directory missing: /root/Datasets/bioscan-5m/bioscan5m/images/cropped_256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.22G/2.22G [12:58<00:00, 2.86MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata CSV file already downloaded and verified\n",
            "Directory missing: /root/Datasets/bioscan-5m/bioscan5m/images/cropped_256/test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.47G/1.47G [42:24<00:00, 576kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata CSV file already downloaded and verified\n",
            "Images already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Bioscan\n",
        "from bioscan_dataset import BIOSCAN5M\n",
        "\n",
        "# Define the transformations for ResNet50\n",
        "# ResNet50_Weights.IMAGENET1K_V2.transforms() provides the recommended transformations for the pre-trained ResNet50\n",
        "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "bioscan_dataset_train = BIOSCAN5M(\"~/Datasets/bioscan-5m\", download=True, split=\"train\", modality=\"image\", target_type=\"family\", target_format=\"index\", transform=transform)\n",
        "bioscan_dataset_test = BIOSCAN5M(\"~/Datasets/bioscan-5m\", download=True, split=\"test\", modality=\"image\", target_type=\"family\", target_format=\"index\", transform=transform)\n",
        "bioscan_dataset_val = BIOSCAN5M(\"~/Datasets/bioscan-5m\", download=True, split=\"val\", modality=\"image\", target_type=\"family\", target_format=\"index\", transform=transform)\n",
        "#To get class from int: idx_to_class\n",
        "\n",
        "#Information about bioscan_dataset: https://github.com/bioscan-ml/dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New Insect Foundation Download Script\n",
        "\n",
        "JSON_PATH = \"/content/drive/MyDrive/Insect-1M-v1.json\"\n",
        "OUT_ROOT = \"/data/insects/images\"\n",
        "MAX_WORKERS = 64   # GCE can handle this easily\n",
        "\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "def extract_real_name(family_string):\n",
        "    if '(' in family_string and ')' in family_string:\n",
        "        return family_string[family_string.find('(')+1:family_string.find(')')]\n",
        "    return family_string\n",
        "\n",
        "with open(JSON_PATH) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = []\n",
        "for r in data[\"insect_records\"]:\n",
        "    if r.get(\"Family\") and r.get(\"image_url\"):\n",
        "        r[\"Family\"] = extract_real_name(r[\"Family\"])\n",
        "        records.append(r)\n",
        "\n",
        "print(f\"Valid records: {len(records)}\")\n",
        "\n",
        "def download_one(idx, record):\n",
        "    family = record[\"Family\"]\n",
        "    url = record[\"image_url\"]\n",
        "\n",
        "    family_dir = os.path.join(OUT_ROOT, family)\n",
        "    os.makedirs(family_dir, exist_ok=True)\n",
        "\n",
        "    out_path = os.path.join(family_dir, f\"{idx:07d}.jpg\")\n",
        "\n",
        "    if os.path.exists(out_path):\n",
        "        return None  # skip if already downloaded\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        img = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
        "        img.save(out_path, \"JPEG\", quality=90)\n",
        "        return out_path\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "with ThreadPoolExecutor(MAX_WORKERS) as ex:\n",
        "    futures = [ex.submit(download_one, i, r) for i, r in enumerate(records)]\n",
        "    for _ in tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass\n",
        "\n",
        "metadata = []\n",
        "for i, r in enumerate(records):\n",
        "    path = f\"{r['Family']}/{i:07d}.jpg\"\n",
        "    metadata.append({\n",
        "        \"path\": path,\n",
        "        \"family\": r[\"Family\"]\n",
        "    })\n",
        "\n",
        "with open(\"/data/insects/metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "with open(\"/data/insects/metadata.json\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "families = sorted(set(x[\"family\"] for x in metadata))\n",
        "family_to_idx = {f: i for i, f in enumerate(families)}\n",
        "\n",
        "class InsectDiskDataset(Dataset):\n",
        "    def __init__(self, root_dir, metadata, family_to_idx, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.metadata = metadata\n",
        "        self.family_to_idx = family_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.metadata[idx]\n",
        "        img_path = os.path.join(self.root_dir, item[\"path\"])\n",
        "        label = self.family_to_idx[item[\"family\"]]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "dataset = InsectDiskDataset(\n",
        "    root_dir=\"/data/insects/images\",\n",
        "    metadata=metadata,\n",
        "    family_to_idx=family_to_idx,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "IF_train_dataset, IF_val_dataset, IF_test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")"
      ],
      "metadata": {
        "id": "lE03SLVvg0I5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3f66750-7df1-42e9-cdfd-afa8e0da81d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid records: 979149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 26165/979149 [32:47<19:54:14, 13.30it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4197053690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4197053690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Insect Foundation Download Script(basic)\n",
        "\n",
        "\n",
        "# Load JSON\n",
        "with open('/content/drive/MyDrive/Insect-1M-v1.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Function to extract scientific name\n",
        "def extract_real_name(family_string):\n",
        "    if '(' in family_string and ')' in family_string:\n",
        "        start = family_string.find('(')\n",
        "        end = family_string.find(')')\n",
        "        return family_string[start+1:end]\n",
        "    return family_string\n",
        "\n",
        "# Filter records without family and image_url\n",
        "filtered_records = []\n",
        "for record in data['insect_records']:\n",
        "    if record.get('Family') and record.get('image_url'):\n",
        "        record['Family'] = extract_real_name(record['Family'])\n",
        "        filtered_records.append(record)\n",
        "\n",
        "\n",
        "print(f\"Original records: {len(data['insect_records'])}\")\n",
        "print(f\"Filtered records: {len(filtered_records)}\")\n",
        "\n",
        "# Create family to index mapping\n",
        "families = sorted(list(set(record['Family'] for record in filtered_records)))\n",
        "family_to_idx = {family: idx for idx, family in enumerate(families)}\n",
        "print(f\"Number of unique families: {len(families)}\")\n",
        "\n",
        "# Custom Dataset class\n",
        "class InsectDataset(Dataset):\n",
        "    def __init__(self, records, family_to_idx, transform=None):\n",
        "        self.records = records\n",
        "        self.family_to_idx = family_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "\n",
        "        # Download and load image\n",
        "        try:\n",
        "            response = requests.get(record['image_url'], timeout=10)\n",
        "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        except Exception as e:\n",
        "            # Return a blank image if download fails\n",
        "            print(f\"Failed to load image {record['image_url']}: {e}\")\n",
        "            img = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        # Apply transform\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Get family label\n",
        "        label = self.family_to_idx[record['Family']]\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Apply ResNet50 transforms\n",
        "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = InsectDataset(filtered_records, family_to_idx, transform)\n",
        "\n",
        "# Split into train (70%), validation (15%), test (15%)\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "IF_train_dataset, IF_val_dataset, IF_test_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"Train: {len(IF_train_dataset)}\")\n",
        "print(f\"Validation: {len(IF_val_dataset)}\")\n",
        "print(f\"Test: {len(IF_test_dataset)}\")"
      ],
      "metadata": {
        "id": "lj8JL5bPqeAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l3jhBijyuzJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bioscan Things"
      ],
      "metadata": {
        "id": "oyllqWBEIXaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Resnet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "bioscan_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "print(f\"ResNet50 model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "EAqDHmIuIzDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20181650-0119-4d74-af9d-f546118214cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 220MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 model loaded on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b472193c",
        "outputId": "582241f3-d9e4-4c1a-c3f9-e85e68501006"
      },
      "source": [
        "#Data loaders\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "NUM_WORKERS = 16\n",
        "\n",
        "# 1. Initialize DataLoader for bioscan_dataset_train\n",
        "bioscan_train_dataloader = DataLoader(\n",
        "    bioscan_dataset_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 2. Initialize DataLoader for bioscan_dataset_val\n",
        "bioscan_val_dataloader = DataLoader(\n",
        "    bioscan_dataset_val,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 3. Initialize DataLoader for bioscan_dataset_test\n",
        "bioscan_test_dataloader = DataLoader(\n",
        "    bioscan_dataset_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {BATCH_SIZE}\")\n",
        "\n",
        "# Verify one batch\n",
        "for images, labels in bioscan_train_dataloader:\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {len(labels)}\")\n",
        "    break # Just take one batch to verify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DataLoader with batch size: 32\n",
            "Batch image shape: torch.Size([32, 3, 224, 224])\n",
            "Batch label shape: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze ResNet50 layers\n",
        "\n",
        "num_classes = 934\n",
        "\n",
        "#Get amt of in features of last layes\n",
        "in_features = bioscan_model.fc.in_features\n",
        "print(f\"Original ResNet50 FC layer in_features: {in_features}\")\n",
        "\n",
        "#Replace Last Layer\n",
        "bioscan_model.fc = nn.Linear(in_features, num_classes)\n",
        "print(f\"Replaced bioscan_model.fc with new nn.Linear layer (in_features={in_features}, out_features={num_classes})\")\n",
        "\n",
        "#Freeze early layers, unfreeze later layers\n",
        "for name, param in bioscan_model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "#Verify\n",
        "trainable_params = [name for name, param in bioscan_model.named_parameters() if param.requires_grad]\n",
        "print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "print(f\"Trainable layers: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXmP91-5psX-",
        "outputId": "ea114c3f-2bdc-45d6-e83c-1c35b77ba2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original ResNet50 FC layer in_features: 2048\n",
            "Replaced bioscan_model.fc with new nn.Linear layer (in_features=2048, out_features=934)\n",
            "Froze all parameters of the base ResNet50 model.\n",
            "Unfroze parameters of the new classification head (bioscan_model.fc).\n",
            "Number of trainable parameters: 2\n",
            "Trainable layers: ['fc.weight', 'fc.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss funtction & optimizer\n",
        "\n",
        "bioscan_criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function (criterion) set to: {bioscan_criterion}\")\n",
        "\n",
        "bioscan_optimizer = optim.Adam(bioscan_model.fc.parameters(), lr=0.001)\n",
        "print(f\"Optimizer set to: {bioscan_optimizer}\")"
      ],
      "metadata": {
        "id": "LQsQGb0lsSEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3629d081-c4e7-4899-ec5e-24f602e36144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function (criterion) set to: CrossEntropyLoss()\n",
            "Optimizer set to: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training loop\n",
        "\n",
        "dataloaders = {'train': bioscan_train_dataloader, 'validation': bioscan_val_dataloader, 'test': bioscan_test_dataloader}\n",
        "\n",
        "def train_bioscan_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long() # Cast labels to torch.Long\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = bioscan_criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    bioscan_optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    bioscan_optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hY9ygBrdcMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "bioscan_model.to(device)\n",
        "bioscan_model = train_bioscan_model(bioscan_model, bioscan_criterion, bioscan_optimizer, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM0V3EZ9BFvU",
        "outputId": "760781c7-bc62-441f-87c5-0a29bf695782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "train loss: 1.3327, acc: 0.6941\n",
            "validation loss: 0.9284, acc: 0.7661\n",
            "Epoch 2/5\n",
            "----------\n",
            "train loss: 0.8272, acc: 0.7849\n",
            "validation loss: 0.8327, acc: 0.7916\n",
            "Epoch 3/5\n",
            "----------\n",
            "train loss: 0.7107, acc: 0.8092\n",
            "validation loss: 0.8115, acc: 0.7924\n",
            "Epoch 4/5\n",
            "----------\n",
            "train loss: 0.6486, acc: 0.8219\n",
            "validation loss: 0.8177, acc: 0.7930\n",
            "Epoch 5/5\n",
            "----------\n",
            "train loss: 0.6091, acc: 0.8307\n",
            "validation loss: 0.8339, acc: 0.7909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "uStzCtlaB6Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model\n",
        "torch.save(bioscan_model.state_dict(), 'models/bioscan_model.pth')"
      ],
      "metadata": {
        "id": "IMhsWSRUB9Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define evaluation/testing loop\n",
        "def evaluate_bioscan_model(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long() # Cast labels to torch\n",
        "            outputs = model(inputs)\n",
        "            loss = bioscan_criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "    print('Test loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6T2-AN4bsHtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model\n",
        "\n",
        "model = bioscan_model\n",
        "dataloader = bioscan_test_dataloader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "evaluate_bioscan_model(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKA5Rfz3nLT0",
        "outputId": "d255b2d6-0aec-45db-e4bf-ff9b9681cde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.3028, acc: 0.6868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insect Foundation Things"
      ],
      "metadata": {
        "id": "VLr380TYzBVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Resnet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "IF_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "print(f\"ResNet50 model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "k_la6OlF3wR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Loaders\n",
        "IF_train_loader = DataLoader(IF_train_dataset, batch_size=512, shuffle=True, num_workers=16)\n",
        "IF_val_loader = DataLoader(IF_val_dataset, batch_size=512, shuffle=False, num_workers=16)\n",
        "IF_test_loader = DataLoader(IF_test_dataset, batch_size=512, shuffle=False, num_workers=16)\n",
        "\n",
        "print(\"\\nDataLoaders created successfully!\")\n",
        "print(f\"Family to index mapping saved with {len(family_to_idx)} classes\")\n",
        "\n",
        "# Verify one batch\n",
        "for images, labels in IF_train_loader:\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {len(labels)}\")\n",
        "    break # Just take one batch to verify"
      ],
      "metadata": {
        "id": "GISYxjRqzAU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze ResNet50 layers\n",
        "\n",
        "num_classes = 1500\n",
        "\n",
        "#Get amt of in features of last layes\n",
        "in_features = IF_model.fc.in_features\n",
        "print(f\"Original ResNet50 FC layer in_features: {in_features}\")\n",
        "\n",
        "#Replace Last Layer\n",
        "IF_model.fc = nn.Linear(in_features, num_classes)\n",
        "print(f\"Replaced bioscan_model.fc with new nn.Linear layer (in_features={in_features}, out_features={num_classes})\")\n",
        "\n",
        "#Freeze early layers, unfreeze later layers\n",
        "for name, param in IF_model.named_parameters():\n",
        "    if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "#Verify\n",
        "trainable_params = [name for name, param in IF_model.named_parameters() if param.requires_grad]\n",
        "print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "print(f\"Trainable layers: {trainable_params}\")"
      ],
      "metadata": {
        "id": "N2_Afo204Pxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss funtction & optimizer\n",
        "\n",
        "IF_criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function (criterion) set to: {IF_criterion}\")\n",
        "\n",
        "IF_optimizer = optim.Adam(IF_model.fc.parameters(), lr=0.001)\n",
        "print(f\"Optimizer set to: {IF_optimizer}\")"
      ],
      "metadata": {
        "id": "ptMOc62Y4WAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training loop\n",
        "\n",
        "dataloaders = {'train': IF_train_loader, 'validation': IF_val_loader, 'test': IF_test_loader}\n",
        "\n",
        "def train_IF_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long() # Cast labels to torch.Long\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = IF_criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    IF_optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    IF_optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MR1MWhgs5F5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "IF_model.to(device)\n",
        "IF_model = train_IF_model(IF_model, IF_criterion, IF_optimizer, num_epochs=5)"
      ],
      "metadata": {
        "id": "QyZfEC2h52jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "S6OmFGhJ6Qvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model\n",
        "torch.save(IF_model.state_dict(), 'models/IF_model.pth')"
      ],
      "metadata": {
        "id": "Vw8y9ySa6Qvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define evaluation/testing loop\n",
        "def evaluate_IF_model(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long() # Cast labels to torch\n",
        "            outputs = model(inputs)\n",
        "            loss = IF_criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "    print('Test loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mMIEtziy6Qvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model\n",
        "\n",
        "model = IF_model\n",
        "dataloader = IF_test_loader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "evaluate_IF_model(model, dataloader)"
      ],
      "metadata": {
        "id": "J2UUWfPL6Qvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Transfer Learning\n",
        "\n",
        "To set up, download data from both, fully train first model"
      ],
      "metadata": {
        "id": "u6xKMxYpo4Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bioscan -> Insect Foundation"
      ],
      "metadata": {
        "id": "Gwi581xprAeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Bioscan Trained ResNet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "resnet50_bioscan_IF_model = resnet50(weights=None).to(device)\n",
        "state_dict = torch.load('models/bioscan_model.pth')\n",
        "state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
        "resnet50_bioscan_IF_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "print(f\"Bioscan model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "8QOu8icBsFBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Loaders\n",
        "IF_train_loader = DataLoader(IF_train_dataset, batch_size=512, shuffle=True, num_workers=16)\n",
        "IF_val_loader = DataLoader(IF_val_dataset, batch_size=512, shuffle=False, num_workers=16)\n",
        "IF_test_loader = DataLoader(IF_test_dataset, batch_size=512, shuffle=False, num_workers=16)\n",
        "\n",
        "print(\"\\nDataLoaders created successfully!\")\n",
        "print(f\"Family to index mapping saved with {len(family_to_idx)} classes\")\n",
        "\n",
        "# Verify one batch\n",
        "for images, labels in IF_train_loader:\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {len(labels)}\")\n",
        "    break # Just take one batch to verify"
      ],
      "metadata": {
        "id": "38WQ6N3NsFBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze ResNet50 layers\n",
        "\n",
        "num_classes = 1500\n",
        "\n",
        "#Get amt of in features of last layes\n",
        "in_features = resnet50_bioscan_IF_model.fc.in_features\n",
        "\n",
        "#Replace Last Layer\n",
        "resnet50_bioscan_IF_model.fc = nn.Linear(in_features, num_classes)\n",
        "print(f\"Replaced bioscan_model.fc with new nn.Linear layer (in_features={in_features}, out_features={num_classes})\")\n",
        "\n",
        "#Freeze early layers, unfreeze later layers\n",
        "for name, param in resnet50_bioscan_IF_model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "#Verify\n",
        "trainable_params = [name for name, param in resnet50_bioscan_IF_model.named_parameters() if param.requires_grad]\n",
        "print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "print(f\"Trainable layers: {trainable_params}\")"
      ],
      "metadata": {
        "id": "WrxsT3uksFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss funtction & optimizer\n",
        "\n",
        "resnet50_bioscan_IF_criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function (criterion) set to: {resnet50_bioscan_IF_criterion}\")\n",
        "\n",
        "resnet50_bioscan_IF_optimizer = optim.Adam([\n",
        "    {\"params\": resnet50_bioscan_IF_model.layer4.parameters(), \"lr\": 5e-5},\n",
        "    {\"params\": resnet50_bioscan_IF_model.fc.parameters(), \"lr\": 1e-3}\n",
        "])\n",
        "\n",
        "print(f\"Optimizer set to: {resnet50_bioscan_IF_optimizer}\")"
      ],
      "metadata": {
        "id": "6VpOdtd5sFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training loop\n",
        "\n",
        "dataloaders = {'train': IF_train_loader, 'validation': IF_val_loader, 'test': IF_test_loader}\n",
        "\n",
        "def train_resnet50_bioscan_IF_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long() # Cast labels to torch.Long\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = resnet50_bioscan_IF_criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    resnet50_bioscan_IF_optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    resnet50_bioscan_IF_optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GHH-ERUysFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "resnet50_bioscan_IF_model.to(device)\n",
        "resnet50_bioscan_IF_model = train_resnet50_bioscan_IF_model(resnet50_bioscan_IF_model, resnet50_bioscan_IF_criterion, resnet50_bioscan_IF_optimizer, num_epochs=5)"
      ],
      "metadata": {
        "id": "zUHQHJVJsFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "L2b5VSbfsFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model\n",
        "torch.save(resnet50_bioscan_IF_model.state_dict(), 'models/resnet50_bioscan_IF_model.pth')"
      ],
      "metadata": {
        "id": "gu9SJPPTsFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define evaluation/testing loop\n",
        "def evaluate_resnet50_bioscan_IF_model(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long() # Cast labels to torch\n",
        "            outputs = model(inputs)\n",
        "            loss = resnet50_bioscan_IF_criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "    print('Test loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pX90bygZsFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model\n",
        "\n",
        "model = resnet50_bioscan_IF_model\n",
        "dataloader = IF_test_loader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "evaluate_resnet50_bioscan_IF_model(model, dataloader)"
      ],
      "metadata": {
        "id": "GF7haoaTsFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insect Foundation -> Bioscan"
      ],
      "metadata": {
        "id": "mzK7kxnNrIv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Resnet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "resnet50_IF_bioscan_model = resnet50(weights=None).to(device)\n",
        "state_dict = torch.load('models/IF_model.pth')\n",
        "state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
        "resnet50_IF_bioscan_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "print(f\"IF model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "nuI8u5WSvjZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqrY6ziFvjZH"
      },
      "source": [
        "#Data loaders\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# 1. Initialize DataLoader for bioscan_dataset_train\n",
        "bioscan_train_dataloader = DataLoader(\n",
        "    bioscan_dataset_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 2. Initialize DataLoader for bioscan_dataset_val\n",
        "bioscan_val_dataloader = DataLoader(\n",
        "    bioscan_dataset_val,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 3. Initialize DataLoader for bioscan_dataset_test\n",
        "bioscan_test_dataloader = DataLoader(\n",
        "    bioscan_dataset_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {BATCH_SIZE}\")\n",
        "\n",
        "# Verify one batch\n",
        "for images, labels in bioscan_train_dataloader:\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {len(labels)}\")\n",
        "    break # Just take one batch to verify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze ResNet50 layers\n",
        "\n",
        "num_classes = 934\n",
        "\n",
        "#Get amt of in features of last layes\n",
        "in_features = resnet50_IF_bioscan_model.fc.in_features\n",
        "print(f\"Original ResNet50 FC layer in_features: {in_features}\")\n",
        "\n",
        "#Replace Last Layer\n",
        "resnet50_IF_bioscan_model.fc = nn.Linear(in_features, num_classes)\n",
        "print(f\"Replaced bioscan_model.fc with new nn.Linear layer (in_features={in_features}, out_features={num_classes})\")\n",
        "\n",
        "#Freeze early layers, unfreeze later layers\n",
        "for name, param in resnet50_IF_bioscan_model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "#Verify\n",
        "trainable_params = [name for name, param in resnet50_IF_bioscan_model.named_parameters() if param.requires_grad]\n",
        "print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "print(f\"Trainable layers: {trainable_params}\")"
      ],
      "metadata": {
        "id": "NcYSkUmOvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss funtction & optimizer\n",
        "\n",
        "resnet50_IF_bioscan_criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function (criterion) set to: {resnet50_IF_bioscan_criterion}\")\n",
        "\n",
        "resnet50_IF_bioscan_optimizer = optim.Adam([\n",
        "    {\"params\": resnet50_IF_bioscan_model.layer4.parameters(), \"lr\": 1e-4},\n",
        "    {\"params\": resnet50_IF_bioscan_model.fc.parameters(), \"lr\": 1e-3}\n",
        "])\n",
        "print(f\"Optimizer set to: {resnet50_IF_bioscan_optimizer}\")"
      ],
      "metadata": {
        "id": "fOrFtjYGvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training loop\n",
        "\n",
        "dataloaders = {'train': bioscan_train_dataloader, 'validation': bioscan_val_dataloader, 'test': bioscan_test_dataloader}\n",
        "\n",
        "def train_resnet50_IF_bioscan_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long() # Cast labels to torch.Long\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = resnet50_IF_bioscan_criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    resnet50_IF_bioscan_optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    resnet50_IF_bioscan_optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "8qN6156RvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "resnet50_IF_bioscan_model.to(device)\n",
        "resnet50_IF_bioscan_model = train_resnet50_IF_bioscan_model(resnet50_IF_bioscan_model, resnet50_IF_bioscan_criterion, resnet50_IF_bioscan_optimizer, num_epochs=5)"
      ],
      "metadata": {
        "id": "6O6zd9yIvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "0kPSIWnnvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model\n",
        "torch.save(resnet50_IF_bioscan_model.state_dict(), 'models/resnet50_IF_bioscan_model.pth')"
      ],
      "metadata": {
        "id": "aYsjMxGTvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define evaluation/testing loop\n",
        "def evaluate_resnet50_IF_bioscan_model(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long() # Cast labels to torch\n",
        "            outputs = model(inputs)\n",
        "            loss = resnet50_IF_bioscan_criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "    print('Test loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WCkng3fpvjZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model\n",
        "\n",
        "model = resnet50_IF_bioscan_model\n",
        "dataloader = bioscan_test_dataloader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "evaluate_resnet50_IF_bioscan_model(model, dataloader)"
      ],
      "metadata": {
        "id": "TSbDSIwGvjZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}