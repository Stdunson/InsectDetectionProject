{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1zcQrp0prydJSfG0VhPdYgPyLC2u6cw0U",
      "authorship_tag": "ABX9TyOzItXn96TdQMVp/MUYKnHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stdunson/InsectDetectionProject/blob/main/InsectThings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for Bioscan Script:\n",
        "1. Download data\n",
        "2. Remove the DNA and BIN data, remove all where family is not present\n",
        "3. Load ResNet\n",
        "4. Split Sets\n",
        "5. Train, Validate\n",
        "6. Test and Gather Data\n",
        "\n",
        "# Steps for Insect Foundation Script:\n",
        "1. Download data\n",
        "2. Parse JSON File, remove all where family is not present\n",
        "3. Load ResNet\n",
        "4. Split Sets\n",
        "5. Train, Validate\n",
        "6. Test and Gather Data\n",
        "\n",
        "#Inconsistencies\n",
        "1. Not every data picture has every taxonomic level, so there has to be a way to filter that during gathering. The lowest level that every image consistently has is order, which is really high. We're gonna focus on family-level\n",
        "2. One dataset is JSON and the other is raw images. Shouldn't matter too much buecause I'm gonna have two different training algorithms\n",
        "3. Bioscan has 934 families while Insect Foundation has 1189(1500 according to code)\n",
        "\n"
      ],
      "metadata": {
        "id": "8wEHouczwKh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installs\n",
        "!pip3 install gdown\n",
        "!pip3 install wget\n",
        "!pip3 install utils\n",
        "!pip3 install bioscan-dataset"
      ],
      "metadata": {
        "id": "wszb2y6VDq31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "isaICYyAlrl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic Imports\n",
        "import os\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "#Doanloading things\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import gdown\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "#Pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Resnet\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n"
      ],
      "metadata": {
        "id": "Fp4Wn6s-ClKh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Downloading"
      ],
      "metadata": {
        "id": "CbWZ_uY3ETvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47WTiIiawIHz"
      },
      "outputs": [],
      "source": [
        "#Bioscan\n",
        "from bioscan_dataset import BIOSCAN5M\n",
        "\n",
        "# Define the transformations for ResNet50\n",
        "# ResNet50_Weights.IMAGENET1K_V2.transforms() provides the recommended transformations for the pre-trained ResNet50\n",
        "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "bioscan_dataset_train = BIOSCAN5M(\"~/Datasets/bioscan-5m\", download=True, split=\"train\", modality=\"image\", target_type=\"family\", target_format=\"index\", transform=transform)\n",
        "bioscan_dataset_test = BIOSCAN5M(\"~/Datasets/bioscan-5m\", download=True, split=\"test\", modality=\"image\", target_type=\"family\", target_format=\"index\", transform=transform)\n",
        "bioscan_dataset_val = BIOSCAN5M(\"~/Datasets/bioscan-5m\", download=True, split=\"val\", modality=\"image\", target_type=\"family\", target_format=\"index\", transform=transform)\n",
        "#To get class from int: idx_to_class\n",
        "\n",
        "#Information about bioscan_dataset: https://github.com/bioscan-ml/dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New Insect Foundation Download Script\n",
        "\n",
        "JSON_PATH = \"/content/drive/MyDrive/Insect-1M-v1.json\"\n",
        "OUT_ROOT = \"/data/insects/images\"\n",
        "MAX_WORKERS = 64   # GCE can handle this easily\n",
        "\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "def extract_real_name(family_string):\n",
        "    if '(' in family_string and ')' in family_string:\n",
        "        return family_string[family_string.find('(')+1:family_string.find(')')]\n",
        "    return family_string\n",
        "\n",
        "with open(JSON_PATH) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = []\n",
        "for r in data[\"insect_records\"]:\n",
        "    if r.get(\"Family\") and r.get(\"image_url\"):\n",
        "        r[\"Family\"] = extract_real_name(r[\"Family\"])\n",
        "        records.append(r)\n",
        "\n",
        "print(f\"Valid records: {len(records)}\")\n",
        "\n",
        "def download_one(idx, record):\n",
        "    family = record[\"Family\"]\n",
        "    url = record[\"image_url\"]\n",
        "\n",
        "    family_dir = os.path.join(OUT_ROOT, family)\n",
        "    os.makedirs(family_dir, exist_ok=True)\n",
        "\n",
        "    out_path = os.path.join(family_dir, f\"{idx:07d}.jpg\")\n",
        "\n",
        "    if os.path.exists(out_path):\n",
        "        return None  # skip if already downloaded\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        img = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
        "        img.save(out_path, \"JPEG\", quality=90)\n",
        "        return out_path\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "with ThreadPoolExecutor(MAX_WORKERS) as ex:\n",
        "    futures = [ex.submit(download_one, i, r) for i, r in enumerate(records)]\n",
        "    for _ in tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass\n",
        "\n",
        "metadata = []\n",
        "for i, r in enumerate(records):\n",
        "    path = f\"{r['Family']}/{i:07d}.jpg\"\n",
        "    metadata.append({\n",
        "        \"path\": path,\n",
        "        \"family\": r[\"Family\"]\n",
        "    })\n",
        "\n",
        "with open(\"/data/insects/metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "with open(\"/data/insects/metadata.json\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "families = sorted(set(x[\"family\"] for x in metadata))\n",
        "family_to_idx = {f: i for i, f in enumerate(families)}\n",
        "\n",
        "class InsectDiskDataset(Dataset):\n",
        "    def __init__(self, root_dir, metadata, family_to_idx, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.metadata = metadata\n",
        "        self.family_to_idx = family_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.metadata[idx]\n",
        "        img_path = os.path.join(self.root_dir, item[\"path\"])\n",
        "        label = self.family_to_idx[item[\"family\"]]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "dataset = InsectDiskDataset(\n",
        "    root_dir=\"/data/insects/images\",\n",
        "    metadata=metadata,\n",
        "    family_to_idx=family_to_idx,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "IF_train_dataset, IF_val_dataset, IF_test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")"
      ],
      "metadata": {
        "id": "lE03SLVvg0I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Insect Foundation Download Script(basic)\n",
        "\n",
        "\n",
        "# Load JSON\n",
        "with open('/content/drive/MyDrive/Insect-1M-v1.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Function to extract scientific name\n",
        "def extract_real_name(family_string):\n",
        "    if '(' in family_string and ')' in family_string:\n",
        "        start = family_string.find('(')\n",
        "        end = family_string.find(')')\n",
        "        return family_string[start+1:end]\n",
        "    return family_string\n",
        "\n",
        "# Filter records without family and image_url\n",
        "filtered_records = []\n",
        "for record in data['insect_records']:\n",
        "    if record.get('Family') and record.get('image_url'):\n",
        "        record['Family'] = extract_real_name(record['Family'])\n",
        "        filtered_records.append(record)\n",
        "\n",
        "\n",
        "print(f\"Original records: {len(data['insect_records'])}\")\n",
        "print(f\"Filtered records: {len(filtered_records)}\")\n",
        "\n",
        "# Create family to index mapping\n",
        "families = sorted(list(set(record['Family'] for record in filtered_records)))\n",
        "family_to_idx = {family: idx for idx, family in enumerate(families)}\n",
        "print(f\"Number of unique families: {len(families)}\")\n",
        "\n",
        "# Custom Dataset class\n",
        "class InsectDataset(Dataset):\n",
        "    def __init__(self, records, family_to_idx, transform=None):\n",
        "        self.records = records\n",
        "        self.family_to_idx = family_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.records[idx]\n",
        "\n",
        "        # Download and load image\n",
        "        try:\n",
        "            response = requests.get(record['image_url'], timeout=10)\n",
        "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        except Exception as e:\n",
        "            # Return a blank image if download fails\n",
        "            print(f\"Failed to load image {record['image_url']}: {e}\")\n",
        "            img = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        # Apply transform\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Get family label\n",
        "        label = self.family_to_idx[record['Family']]\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Apply ResNet50 transforms\n",
        "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = InsectDataset(filtered_records, family_to_idx, transform)\n",
        "\n",
        "# Split into train (70%), validation (15%), test (15%)\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "IF_train_dataset, IF_val_dataset, IF_test_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset splits:\")\n",
        "print(f\"Train: {len(IF_train_dataset)}\")\n",
        "print(f\"Validation: {len(IF_val_dataset)}\")\n",
        "print(f\"Test: {len(IF_test_dataset)}\")"
      ],
      "metadata": {
        "id": "lj8JL5bPqeAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l3jhBijyuzJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bioscan Things"
      ],
      "metadata": {
        "id": "oyllqWBEIXaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Resnet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "bioscan_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "print(f\"ResNet50 model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "EAqDHmIuIzDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20181650-0119-4d74-af9d-f546118214cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 220MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet50 model loaded on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b472193c",
        "outputId": "582241f3-d9e4-4c1a-c3f9-e85e68501006"
      },
      "source": [
        "#Data loaders\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# 1. Initialize DataLoader for bioscan_dataset_train\n",
        "bioscan_train_dataloader = DataLoader(\n",
        "    bioscan_dataset_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 2. Initialize DataLoader for bioscan_dataset_val\n",
        "bioscan_val_dataloader = DataLoader(\n",
        "    bioscan_dataset_val,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# 3. Initialize DataLoader for bioscan_dataset_test\n",
        "bioscan_test_dataloader = DataLoader(\n",
        "    bioscan_dataset_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(f\"Created DataLoader with batch size: {BATCH_SIZE}\")\n",
        "\n",
        "# Verify one batch\n",
        "for images, labels in bioscan_train_dataloader:\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {len(labels)}\")\n",
        "    break # Just take one batch to verify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created DataLoader with batch size: 32\n",
            "Batch image shape: torch.Size([32, 3, 224, 224])\n",
            "Batch label shape: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze ResNet50 layers\n",
        "\n",
        "num_classes = 934\n",
        "\n",
        "#Get amt of in features of last layes\n",
        "in_features = bioscan_model.fc.in_features\n",
        "print(f\"Original ResNet50 FC layer in_features: {in_features}\")\n",
        "\n",
        "#Replace Last Layer\n",
        "bioscan_model.fc = nn.Linear(in_features, num_classes)\n",
        "print(f\"Replaced bioscan_model.fc with new nn.Linear layer (in_features={in_features}, out_features={num_classes})\")\n",
        "\n",
        "#Freeze parameters of pre-trained bioscan_model, then unfreeze only the parameters of classification head\n",
        "for param in bioscan_model.parameters():\n",
        "    param.requires_grad = False\n",
        "print(\"Froze all parameters of the base ResNet50 model.\")\n",
        "for param in bioscan_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "print(\"Unfroze parameters of the new classification head (bioscan_model.fc).\")\n",
        "\n",
        "#Verify\n",
        "trainable_params = [name for name, param in bioscan_model.named_parameters() if param.requires_grad]\n",
        "print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "print(f\"Trainable layers: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXmP91-5psX-",
        "outputId": "ea114c3f-2bdc-45d6-e83c-1c35b77ba2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original ResNet50 FC layer in_features: 2048\n",
            "Replaced bioscan_model.fc with new nn.Linear layer (in_features=2048, out_features=934)\n",
            "Froze all parameters of the base ResNet50 model.\n",
            "Unfroze parameters of the new classification head (bioscan_model.fc).\n",
            "Number of trainable parameters: 2\n",
            "Trainable layers: ['fc.weight', 'fc.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss funtction & optimizer\n",
        "\n",
        "bioscan_criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function (criterion) set to: {bioscan_criterion}\")\n",
        "\n",
        "bioscan_optimizer = optim.Adam(bioscan_model.fc.parameters(), lr=0.001)\n",
        "print(f\"Optimizer set to: {bioscan_optimizer}\")"
      ],
      "metadata": {
        "id": "LQsQGb0lsSEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3629d081-c4e7-4899-ec5e-24f602e36144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function (criterion) set to: CrossEntropyLoss()\n",
            "Optimizer set to: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training loop\n",
        "\n",
        "dataloaders = {'train': bioscan_train_dataloader, 'validation': bioscan_val_dataloader, 'test': bioscan_test_dataloader}\n",
        "\n",
        "def train_bioscan_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long() # Cast labels to torch.Long\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hY9ygBrdcMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "bioscan_model.to(device)\n",
        "bioscan_model = train_bioscan_model(bioscan_model, bioscan_criterion, bioscan_optimizer, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM0V3EZ9BFvU",
        "outputId": "760781c7-bc62-441f-87c5-0a29bf695782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "train loss: 1.3327, acc: 0.6941\n",
            "validation loss: 0.9284, acc: 0.7661\n",
            "Epoch 2/5\n",
            "----------\n",
            "train loss: 0.8272, acc: 0.7849\n",
            "validation loss: 0.8327, acc: 0.7916\n",
            "Epoch 3/5\n",
            "----------\n",
            "train loss: 0.7107, acc: 0.8092\n",
            "validation loss: 0.8115, acc: 0.7924\n",
            "Epoch 4/5\n",
            "----------\n",
            "train loss: 0.6486, acc: 0.8219\n",
            "validation loss: 0.8177, acc: 0.7930\n",
            "Epoch 5/5\n",
            "----------\n",
            "train loss: 0.6091, acc: 0.8307\n",
            "validation loss: 0.8339, acc: 0.7909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "uStzCtlaB6Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model\n",
        "torch.save(bioscan_model.state_dict(), 'models/bioscan_model.pth')"
      ],
      "metadata": {
        "id": "IMhsWSRUB9Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define evaluation/testing loop\n",
        "def evaluate_bioscan_model(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long() # Cast labels to torch\n",
        "            outputs = model(inputs)\n",
        "            loss = bioscan_criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "    print('Test loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6T2-AN4bsHtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model\n",
        "\n",
        "model = bioscan_model\n",
        "dataloader = bioscan_test_dataloader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "evaluate_bioscan_model(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKA5Rfz3nLT0",
        "outputId": "d255b2d6-0aec-45db-e4bf-ff9b9681cde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1.3028, acc: 0.6868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insect Foundation Things"
      ],
      "metadata": {
        "id": "VLr380TYzBVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Resnet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "IF_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
        "print(f\"ResNet50 model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "k_la6OlF3wR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Loaders\n",
        "IF_train_loader = DataLoader(IF_train_dataset, batch_size=512, shuffle=True, num_workers=16)\n",
        "IF_val_loader = DataLoader(IF_val_dataset, batch_size=512, shuffle=False, num_workers=16)\n",
        "IF_test_loader = DataLoader(IF_test_dataset, batch_size=512, shuffle=False, num_workers=16)\n",
        "\n",
        "print(\"\\nDataLoaders created successfully!\")\n",
        "print(f\"Family to index mapping saved with {len(family_to_idx)} classes\")\n",
        "\n",
        "# Verify one batch\n",
        "for images, labels in IF_train_loader:\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {len(labels)}\")\n",
        "    break # Just take one batch to verify"
      ],
      "metadata": {
        "id": "GISYxjRqzAU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze ResNet50 layers\n",
        "\n",
        "num_classes = 1500\n",
        "\n",
        "#Get amt of in features of last layes\n",
        "in_features = IF_model.fc.in_features\n",
        "print(f\"Original ResNet50 FC layer in_features: {in_features}\")\n",
        "\n",
        "#Replace Last Layer\n",
        "IF_model.fc = nn.Linear(in_features, num_classes)\n",
        "print(f\"Replaced bioscan_model.fc with new nn.Linear layer (in_features={in_features}, out_features={num_classes})\")\n",
        "\n",
        "#Freeze parameters of pre-trained bioscan_model, then unfreeze only the parameters of classification head\n",
        "for param in IF_model.parameters():\n",
        "    param.requires_grad = False\n",
        "print(\"Froze all parameters of the base ResNet50 model.\")\n",
        "for param in IF_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "print(\"Unfroze parameters of the new classification head (bioscan_model.fc).\")\n",
        "\n",
        "#Verify\n",
        "trainable_params = [name for name, param in IF_model.named_parameters() if param.requires_grad]\n",
        "print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "print(f\"Trainable layers: {trainable_params}\")"
      ],
      "metadata": {
        "id": "N2_Afo204Pxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss funtction & optimizer\n",
        "\n",
        "IF_criterion = nn.CrossEntropyLoss()\n",
        "print(f\"Loss function (criterion) set to: {IF_criterion}\")\n",
        "\n",
        "IF_optimizer = optim.Adam(IF_model.fc.parameters(), lr=0.001)\n",
        "print(f\"Optimizer set to: {IF_optimizer}\")"
      ],
      "metadata": {
        "id": "ptMOc62Y4WAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training loop\n",
        "\n",
        "dataloaders = {'train': IF_train_loader, 'validation': IF_val_loader, 'test': IF_test_loader}\n",
        "\n",
        "def train_IF_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long() # Cast labels to torch.Long\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "MR1MWhgs5F5C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "IF_model.to(device)\n",
        "IF_model = train_IF_model(IF_model, IF_criterion, IF_optimizer, num_epochs=5)"
      ],
      "metadata": {
        "id": "QyZfEC2h52jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "S6OmFGhJ6Qvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Model\n",
        "torch.save(IF_model.state_dict(), 'models/IF_model.pth')"
      ],
      "metadata": {
        "id": "Vw8y9ySa6Qvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define evaluation/testing loop\n",
        "def evaluate_IF_model(model, dataloader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device).long() # Cast labels to torch\n",
        "            outputs = model(inputs)\n",
        "            loss = IF_criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "    print('Test loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mMIEtziy6Qvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate Model\n",
        "\n",
        "model = IF_model\n",
        "dataloader = IF_test_loader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "evaluate_IF_model(model, dataloader)"
      ],
      "metadata": {
        "id": "J2UUWfPL6Qvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}